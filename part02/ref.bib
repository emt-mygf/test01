@article{lample2016neural,
	title={Neural architectures for named entity recognition},
	author={Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
	journal={arXiv preprint arXiv:1603.01360},
	year={2016}
}

@article{yan2019tener,
	title={Tener: Adapting transformer encoder for name entity recognition},
	author={Yan, Hang and Deng, Bocao and Li, Xiaonan and Qiu, Xipeng},
	journal={arXiv preprint arXiv:1911.04474},
	year={2019}
}

@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}

@article{zhang2018chinese,
	title={Chinese ner using lattice lstm},
	author={Zhang, Yue and Yang, Jie},
	journal={arXiv preprint arXiv:1805.02023},
	year={2018}
}

@article{li2020flat,
	title={FLAT: Chinese NER Using Flat-Lattice Transformer},
	author={Li, Xiaonan and Yan, Hang and Qiu, Xipeng and Huang, Xuanjing},
	journal={arXiv preprint arXiv:2004.11795},
	year={2020}
}

@article{peng2019simplify,
	title={Simplify the Usage of Lexicon in Chinese NER},
	author={Peng, Minlong and Ma, Ruotian and Zhang, Qi and Huang, Xuanjing},
	journal={arXiv preprint arXiv:1908.05969},
	year={2019}
}

@article{yamada2020luke,
	title={LUKE: deep contextualized entity representations with entity-aware self-attention},
	author={Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Matsumoto, Yuji},
	journal={arXiv preprint arXiv:2010.01057},
	year={2020}
}

@inproceedings{yang2019xlnet,
	title={Xlnet: Generalized autoregressive pretraining for language understanding},
	author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	booktitle={Advances in neural information processing systems},
	pages={5753--5763},
	year={2019}
}