\begin{thebibliography}{1}

\bibitem{lample2016neural}
G.~Lample, M.~Ballesteros, S.~Subramanian, K.~Kawakami, and C.~Dyer, ``Neural
  architectures for named entity recognition,'' {\em arXiv preprint
  arXiv:1603.01360}, 2016.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' {\em arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{yan2019tener}
H.~Yan, B.~Deng, X.~Li, and X.~Qiu, ``Tener: Adapting transformer encoder for
  name entity recognition,'' {\em arXiv preprint arXiv:1911.04474}, 2019.

\bibitem{zhang2018chinese}
Y.~Zhang and J.~Yang, ``Chinese ner using lattice lstm,'' {\em arXiv preprint
  arXiv:1805.02023}, 2018.

\bibitem{li2020flat}
X.~Li, H.~Yan, X.~Qiu, and X.~Huang, ``Flat: Chinese ner using flat-lattice
  transformer,'' {\em arXiv preprint arXiv:2004.11795}, 2020.

\bibitem{peng2019simplify}
M.~Peng, R.~Ma, Q.~Zhang, and X.~Huang, ``Simplify the usage of lexicon in
  chinese ner,'' {\em arXiv preprint arXiv:1908.05969}, 2019.

\bibitem{yamada2020luke}
I.~Yamada, A.~Asai, H.~Shindo, H.~Takeda, and Y.~Matsumoto, ``Luke: deep
  contextualized entity representations with entity-aware self-attention,''
  {\em arXiv preprint arXiv:2010.01057}, 2020.

\bibitem{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~R. Salakhutdinov, and Q.~V. Le,
  ``Xlnet: Generalized autoregressive pretraining for language understanding,''
  in {\em Advances in neural information processing systems}, pp.~5753--5763,
  2019.

\end{thebibliography}
